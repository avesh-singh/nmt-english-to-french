from model import *
from util import *
from data import *
import pickle
import random

# Setup the model and train
device = "cuda:0" if torch.cuda.is_available() else "cpu"
HIDDEN_SIZE = 512
EMBEDDING_SIZE = 256
EPOCHS = 15
NUM_LAYERS = 2
DROPOUT = 0.3
teacher_forcing_ratio = 0.4

(train_data, valid_data, test_data), src, tgt = prepare_data()
encoder = EncoderRNN(EMBEDDING_SIZE, len(src.vocab), HIDDEN_SIZE, NUM_LAYERS, DROPOUT)
decoder = DecoderRNN(EMBEDDING_SIZE, len(tgt.vocab), HIDDEN_SIZE, len(tgt.vocab), NUM_LAYERS, DROPOUT)
model = Seq2Seq(encoder, decoder, device).to(device)
model.apply(init_weights)
model_optimizer = torch.optim.Adam(model.parameters())
criterion = torch.nn.CrossEntropyLoss(ignore_index=tgt.vocab.stoi[tgt.pad_token])


def train(batch, clip):
    model_optimizer.zero_grad()
    src = batch.src
    tgt = batch.trg

    output = model(src, tgt, teacher_forcing_ratio)

    output_dim = output.shape[-1]
    output = output[1:].view(-1, output_dim)
    tgt = tgt[1:].view(-1)
    loss = criterion(output, tgt)
    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), clip)
    model_optimizer.step()
    return loss.item() / output_dim


def train_iters():
    start = time.time()
    plot_loss=[]
    print_loss_total = 0
    plot_loss_total = 0
    print_every = 1500
    plot_every = 100
    clip = 1
    best_valid_loss = float('inf')
    for epoch in range(1, EPOCHS + 1):
        train_loss = 0
        epoch_time = time.time()
        model.train()
        for i, batch in enumerate(train_data):
            loss = train(batch, clip)
            train_loss += loss
            print_loss_total += loss
            plot_loss_total += loss
            if i % print_every == 0 and i != 0:
                print_loss_avg = print_loss_total / (i + 1)
                print_loss_total = 0
                print(f'{time_since(epoch_time, (i+1) / len(train_data))} ({i} {round(i/ len(train_data) * 100, 4)}%'
                      f' {round(print_loss_avg, 4)})')

            if i % plot_every == 0 and i != 0:
                plot_loss_avg = plot_loss_total / plot_every
                plot_loss_total = 0
                plot_loss.append(plot_loss_avg)
        valid_loss = evaluate(model, valid_data)
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), 'model/model.pt')
        print(f"Epoch: {epoch} | Time: {time_since(start, epoch / EPOCHS)}")
        print(f"\tTrain loss: {train_loss:.3f}      | Train ppl: {math.exp(train_loss):7.3f}")
        print(f"\tValidation loss: {valid_loss:.3f} | Validation ppl: {math.exp(valid_loss):7.3f}")
    return plot_loss


def evaluate(model, data):
    model.eval()
    loss = 0
    with torch.no_grad():
        for i, batch in enumerate(data):
            src = batch.src
            tgt = batch.trg
            output = model(src, tgt)
            output_dim = output.size(-1)
            output = output[1:].view(-1, output_dim)
            tgt = tgt[1:].view(-1)
            loss += criterion(output, tgt).item()
    return loss / len(data)


if __name__ == "__main__":
    losses = train_iters()
    show_plot(losses)
    model.load_state_dict(torch.load('model/model.pt'))
    test_loss = evaluate(model, test_data)
    print(f"\n\nTesting loss: {test_loss:.3f} | Testing ppl: {math.exp(test_loss):7.3f}")
